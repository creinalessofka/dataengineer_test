{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: SQL\n",
    "\n",
    "There are two tables in the included database\n",
    "\n",
    "1. train_table\n",
    "2. test_table\n",
    "\n",
    "\n",
    "IMPORTANT: No other libraries are allowed to solve this tests only SQL Queries allowed\n",
    "\n",
    "* pandas methods are not is not allowed\n",
    "* sqlalchemy is not allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only library allowed do not modify\n",
    "import pandas as pd\n",
    "from src.sql import execute_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example query inside docstring\n",
    "execute_query(\"\"\"\n",
    "        SELECT * \n",
    "        FROM train_table\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example query inside docstring\n",
    "execute_query(\"\"\"\n",
    "        SELECT * \n",
    "        FROM test_table\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question 1: Basic SQL\n",
    "Write a SQL statement from table <i>train_table</i> to obtain the top 10 <i>locations</i> people twit from (in descending order)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EXAMPLE SOLUTION:\n",
    "\n",
    "   UserName          Location  count\n",
    "0     11756       Doha, Qatar     10\n",
    "..\n",
    "8       450    The Colony, TX      9\n",
    "9     14693  Brighton Lass ??      8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"\"\"\n",
    "SELECT\n",
    "    UserName, \n",
    "    Location,\n",
    "    COUNT(*) AS tweet_count\n",
    "FROM train_table\n",
    "WHERE Location IS NOT NULL \n",
    "GROUP BY Location\n",
    "ORDER BY tweet_count DESC\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_1 = execute_query(query_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Intermediate SQL\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You are tasked to build a dataset that contains the count of Sentiment values as columns for every UserName.\n",
    "\n",
    "The dataset must contain the following columns and name convention:\n",
    "    0. UserName (int)\n",
    "    1. pos_count (int)\n",
    "    2. neg_count (int)\n",
    "    3. extremely_pos_count (int)\n",
    "    4. extremely_neg_count (int)\n",
    "\n",
    "EXAMPLE SOLUTION:\n",
    "\n",
    "\n",
    "   UserName  pos_count  neg_count  extremely_pos_count  extremely_neg_count\n",
    "0     14675          1          7                    1                    0\n",
    "1     12457          1          5                    0                    0\n",
    "...\n",
    "9     14405          0          4                    0                    1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = \"\"\"\n",
    "SELECT\n",
    "    UserName,\n",
    "    SUM(CASE WHEN Sentiment = 'Positive' THEN 1 ELSE 0 END) AS pos_count,\n",
    "    SUM(CASE WHEN Sentiment = 'Negative' THEN 1 ELSE 0 END) AS neg_count,\n",
    "    SUM(CASE WHEN Sentiment = 'Extremely Positive' THEN 1 ELSE 0 END) AS extremely_pos_count,\n",
    "    SUM(CASE WHEN Sentiment = 'Extremely Negative' THEN 1 ELSE 0 END) AS extremely_neg_count\n",
    "FROM train_table\n",
    "GROUP BY UserName;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2 = execute_query(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Medium SQL"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "The sales department asks you to plot the positive sentiment rate in time\n",
    "\n",
    "The positive sentiment rate is defined as: \n",
    "\n",
    "    (count of positives + count of extremely positives) / [(count of negatives + count of extremely negatives) + (count of positives + count of extremely positives)]\n",
    "\n",
    "requirement:\n",
    "\n",
    "1. positive rate column most be named pos_rate\n",
    "\n",
    "\n",
    "\n",
    "EXAMPLE SOLUTION:\n",
    "\n",
    "\n",
    "\ttweetAt\t    pos_rate\n",
    "0\t18-03-2020\t1.000000\n",
    "1\t21-03-2020\t1.000000\n",
    "...\n",
    "4\t18-03-2020\t0.777778"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_3 = \"\"\"\n",
    "--(count of positives + count of extremely positives) / [(count of negatives + count of extremely negatives) + (count of positives + count of extremely positives)]\n",
    "SELECT\n",
    "    TweetAt,\n",
    "    (\n",
    "        (\n",
    "            SUM(CASE WHEN Sentiment = 'Positive' THEN 1 ELSE 0 END)\n",
    "            + SUM(CASE WHEN Sentiment = 'Extremely Positive' THEN 1 ELSE 0 END)\n",
    "        ) * 1.0\n",
    "            /\n",
    "        (\n",
    "            SUM(CASE WHEN Sentiment IN ('Positive', 'Extremely Positive') THEN 1 ELSE 0 END)\n",
    "            + SUM(CASE WHEN Sentiment IN ('Negative', 'Extremely Negative') THEN 1 ELSE 0 END)\n",
    "        )\n",
    "    ) AS pos_rate\n",
    "FROM train_table\n",
    "GROUP BY TweetAt\n",
    "ORDER BY TweetAt;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3 = execute_query(query_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: SQL + pandas + plotting"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You are tasked to deliver a simple plot a time series of the answer of Question 3\n",
    "\n",
    "Requirements:\n",
    "    1. the proceedure most take the result_3 variable as input and return nothing\n",
    "    2. the plot most be resampled by day aggregated by the mean\n",
    "    3. no other library like matplotlib can be used only pandas plot interface pd.Series.plot()\n",
    "    4. the date column from result_3 most be parsed into datetime format using pandas built in method.\n",
    "    5. the proceedure must not transform or modify the original result_3 variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Expected result:</p>\n",
    "<img src=\"data/plot_result.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df : pd.DataFrame):\n",
    "    #TODO: YOUR CODE GOES HERE\n",
    "    # Make a copy so we don't modify the original result_3 DataFrame\n",
    "    temp_df = df.copy()\n",
    "\n",
    "    # Convert the tweetAt column to datetime (requirement #4)\n",
    "    temp_df[\"TweetAt\"] = pd.to_datetime(temp_df[\"TweetAt\"], format=\"%d-%m-%Y\")\n",
    "\n",
    "    # Set the datetime column as the index\n",
    "    temp_df.set_index('TweetAt', inplace=True)\n",
    "\n",
    "    # Resample by day, aggregating by mean (requirement #2)\n",
    "    daily_df = temp_df.resample('D').mean()\n",
    "\n",
    "    # Plot the pos_rate time series using only pandas plot interface (requirement #3)\n",
    "    daily_df['pos_rate'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(result_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Object Oriented Programming + Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You are tasked to build a csv data reader from scratch.\n",
    "As a major constraint you cannot use any third party library like pandas or another csv reader. \n",
    "***Only built in standard python libraries are allowed.***\n",
    "\n",
    "\n",
    "The problem is that the csv data provided has many mistakes and the user requires to keep the amount of data possible.\n",
    "The user does not wants to drop rows or columns with mistakes but may replace mistakes with \"NaN\" values\n",
    "\n",
    "REQUIREMENTS:\n",
    "\n",
    "I. the csv must be contained inside a class named Data and it must have:\n",
    "    1. must persist the loaded csv data in a local parameter called data\n",
    "    2. infer_dtypes(data_object: object) : to infer the data types of each column distinguishing between (date, integer, or float)\n",
    "    3. describe() method that returns basic statistics of each numerical column including mean, standard deviation, maximum value, minimum value.\n",
    "II. You most build a class called DataReader with the following required methods:\n",
    "    1. read_csv(file_path : Str) : to load the raw csv file and return a Data class\n",
    "\n",
    "\n",
    "You can assume:\n",
    "    1. all columns have different errors at different rows\n",
    "    2. you can delete rows ONLY if necesary and you need to comment why\n",
    "    3. the more rows you drop the less points you will obtain\n",
    "\n",
    "\n",
    "If it is not specified in this instructions you are free to assume but explain your assumptions.\n",
    "Use best practices and comment your code as best as possible.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "\n",
    "class Data:\n",
    "    \"\"\"\n",
    "    This class holds the CSV data internally and provides:\n",
    "      1) a mechanism to infer dtypes of each column,\n",
    "      2) a method to describe() numerical columns (mean, std, min, max).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, header, rows):\n",
    "        \"\"\"\n",
    "        :param header: list of column names\n",
    "        :param rows: list of lists, where each sublist is a row of CSV data\n",
    "        \"\"\"\n",
    "        self.header = header\n",
    "        self.data = {}  # This dict will map {column_name: [col_values]}\n",
    "        self.dtypes = {}  # Will store the inferred data types per column\n",
    "\n",
    "        # Initialize self.data with an empty list for each column\n",
    "        for col_name in self.header:\n",
    "            self.data[col_name] = []\n",
    "\n",
    "        # Populate self.data by columns\n",
    "        for row in rows:\n",
    "            for col_name, col_val in zip(self.header, row):\n",
    "                self.data[col_name].append(col_val)\n",
    "\n",
    "        # Infer data types after the data has been stored\n",
    "        self.dtypes = self.infer_dtypes()\n",
    "    \n",
    "    def infer_dtypes(self) -> dict:\n",
    "        \"\"\"\n",
    "        Infers the data types of each column distinguishing between date, integer, or float.\n",
    "        If a column fails all numeric/date checks, it will be considered 'string'.\n",
    "\n",
    "        Returns: dict of the form { column_name: \"date\"/\"integer\"/\"float\"/\"string\" }\n",
    "        \"\"\"\n",
    "        inferred = {}\n",
    "\n",
    "        for col_name, values in self.data.items():\n",
    "            # Attempt to parse as date (we assume a fixed known format, e.g. '%Y-%m-%d',\n",
    "            # or we can try multiple formats)\n",
    "            # We'll attempt integer => float => date in some logical order\n",
    "            # or we can do date => integer => float, depending on your preference.\n",
    "            # \n",
    "            # For the sake of demonstration: integer -> float -> date -> string.\n",
    "            # If a single value does not fit the candidate type, we move to the next type.\n",
    "\n",
    "            # We'll store which type we think it is, defaulting to \"string\".\n",
    "            # If at least 90% of values fit a type, you might consider that the type, etc.\n",
    "            # But for simplicity, let's be strict and check all rows.\n",
    "\n",
    "            # 1) Check integer possibility\n",
    "            if self._all_integers(values):\n",
    "                inferred[col_name] = \"integer\"\n",
    "                continue\n",
    "\n",
    "            # 2) Check float possibility\n",
    "            if self._all_floats(values):\n",
    "                inferred[col_name] = \"float\"\n",
    "                continue\n",
    "\n",
    "            # 3) Check date possibility\n",
    "            #   In case your CSV dates have multiple possible formats, you can\n",
    "            #   systematically try them, e.g., \"%Y-%m-%d\", \"%d-%m-%Y\", etc.\n",
    "            #   We'll assume a single format for demonstration. \n",
    "            if self._all_dates(values, date_format=\"%d-%m-%Y\"):\n",
    "                inferred[col_name] = \"date\"\n",
    "                continue\n",
    "\n",
    "            # If none matched, store as string\n",
    "            inferred[col_name] = \"string\"\n",
    "\n",
    "        return inferred\n",
    "\n",
    "    def describe(self) -> dict:\n",
    "        \"\"\"\n",
    "        Returns basic statistics of each numerical column: mean, std, max, min.\n",
    "\n",
    "        Example output:\n",
    "            {\n",
    "                \"col_name\": {\n",
    "                    \"mean\": float,\n",
    "                    \"std\": float,\n",
    "                    \"min\": float,\n",
    "                    \"max\": float\n",
    "                },\n",
    "                ...\n",
    "            }\n",
    "        \"\"\"\n",
    "        stats = {}\n",
    "\n",
    "        for col_name, col_dtype in self.dtypes.items():\n",
    "            if col_dtype in (\"integer\", \"float\"):\n",
    "                # Gather non-NaN numeric values\n",
    "                numeric_values = []\n",
    "                for val in self.data[col_name]:\n",
    "                    # We may have replaced errors with 'NaN' (string), so skip them\n",
    "                    if val == \"NaN\":\n",
    "                        continue\n",
    "                    if isinstance(val, int) or isinstance(val, float):\n",
    "                        numeric_values.append(val)\n",
    "                    else:\n",
    "                        # If stored as string but declared numeric, try to parse\n",
    "                        try:\n",
    "                            numeric_values.append(float(val))\n",
    "                        except ValueError:\n",
    "                            continue  # skip if can't parse\n",
    "\n",
    "                if len(numeric_values) == 0:\n",
    "                    # If no valid numeric data, skip the stats\n",
    "                    continue\n",
    "\n",
    "                col_mean = sum(numeric_values) / len(numeric_values)\n",
    "                # Calculate standard deviation\n",
    "                # population std or sample std? We'll assume sample here\n",
    "                # but the user can adapt as needed.\n",
    "                variance = sum((x - col_mean)**2 for x in numeric_values) / (len(numeric_values) - 1) \\\n",
    "                           if len(numeric_values) > 1 else 0.0\n",
    "                col_std = sqrt(variance)\n",
    "                col_min = min(numeric_values)\n",
    "                col_max = max(numeric_values)\n",
    "\n",
    "                stats[col_name] = {\n",
    "                    \"mean\": col_mean,\n",
    "                    \"std\": col_std,\n",
    "                    \"min\": col_min,\n",
    "                    \"max\": col_max\n",
    "                }\n",
    "        return stats\n",
    "\n",
    "    @staticmethod\n",
    "    def _all_integers(values):\n",
    "        \"\"\"\n",
    "        Return True if every non-\"NaN\" value can be parsed as integer\n",
    "        (or is already an integer).\n",
    "        Otherwise False.\n",
    "        \"\"\"\n",
    "        for val in values:\n",
    "            if val == \"NaN\":\n",
    "                continue  # skip\n",
    "            try:\n",
    "                int(val)\n",
    "            except ValueError:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def _all_floats(values):\n",
    "        \"\"\"\n",
    "        Return True if every non-\"NaN\" value can be parsed as float\n",
    "        (or is already float).\n",
    "        Otherwise False.\n",
    "        \"\"\"\n",
    "        for val in values:\n",
    "            if val == \"NaN\":\n",
    "                continue  # skip\n",
    "            try:\n",
    "                float(val)\n",
    "            except ValueError:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def _all_dates(values, date_format=\"%Y-%m-%d\"):\n",
    "        \"\"\"\n",
    "        Return True if every non-\"NaN\" value can be parsed as a date\n",
    "        using the given format. Otherwise False.\n",
    "        \"\"\"\n",
    "        for val in values:\n",
    "            if val == \"NaN\":\n",
    "                continue  # skip\n",
    "            try:\n",
    "                datetime.strptime(val, date_format)\n",
    "            except ValueError:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "class DataReader:\n",
    "    \"\"\"\n",
    "    This class is responsible for reading a CSV file (with potential errors) into a Data object,\n",
    "    replacing errors with 'NaN' if possible, or dropping rows ONLY if absolutely necessary.\n",
    "\n",
    "    read_csv(file_path: str) -> Data\n",
    "        1) read the CSV,\n",
    "        2) parse it,\n",
    "        3) clean up errors by replacing them with 'NaN' where possible,\n",
    "        4) return a Data instance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def read_csv(self, file_path: str) -> Data:\n",
    "        \"\"\"\n",
    "        Reads the raw CSV file, attempts to parse rows/columns, \n",
    "        and returns a Data object that contains the data.\n",
    "        \n",
    "        :param file_path: path to the CSV file\n",
    "        :return: Data object with the loaded data\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        header = []\n",
    "\n",
    "        # We’ll assume the first row is the header.  \n",
    "        # We'll parse each subsequent row's columns, \n",
    "        # replacing known mistakes with 'NaN' if we cannot parse them at all.\n",
    "        with open(file_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            first_line = True\n",
    "\n",
    "            for row in reader:\n",
    "                # If it's the header, store it and continue\n",
    "                if first_line:\n",
    "                    header = row\n",
    "                    first_line = False\n",
    "                    continue\n",
    "\n",
    "                # If row has fewer or more columns than the header, \n",
    "                # we can attempt to fill the missing columns with 'NaN' \n",
    "                # or shorten if there's an obvious extra delimiter.\n",
    "                # Minimal approach: we'll align to the length of header.\n",
    "                if len(row) < len(header):\n",
    "                    # Fill missing columns with NaN\n",
    "                    row += [\"NaN\"] * (len(header) - len(row))\n",
    "                elif len(row) > len(header):\n",
    "                    # Truncate extra columns\n",
    "                    # NOTE: We do this carefully because the user doesn't want\n",
    "                    # to lose data, but there's no robust way if the CSV is heavily malformed.\n",
    "                    # We'll just keep the first N columns for alignment with the header\n",
    "                    row = row[:len(header)]\n",
    "\n",
    "                # Replace empty strings with 'NaN' for clarity\n",
    "                cleaned_row = [col if col.strip() != \"\" else \"NaN\" for col in row]\n",
    "\n",
    "                rows.append(cleaned_row)\n",
    "\n",
    "        # Return the Data object\n",
    "        return Data(header, rows)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use it:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ---------------------------------------------------\n",
    "# Suppose you have the classes Data and DataReader \n",
    "# either in the same Python file or properly imported:\n",
    "# from my_data_lib import Data, DataReader\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# 1) Instantiate the reader\n",
    "reader = DataReader()\n",
    "\n",
    "# 2) Read the CSV file into a Data object\n",
    "data_object = reader.read_csv(\"example.csv\") ##malformed_dataset.csv\n",
    "\n",
    "# 3) Look at the inferred data types\n",
    "print(\"Inferred dtypes:\", data_object.dtypes)\n",
    "\n",
    "# 4) Print out basic statistics for numeric columns\n",
    "stats = data_object.describe()\n",
    "print(\"\\nDescriptive statistics for numeric columns:\")\n",
    "for col_name, col_stats in stats.items():\n",
    "    print(f\"{col_name}: {col_stats}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d188dfcbde1e480b71c66e69b52162f1721e9379a92df673941419cd9e5e1b3f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
